{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Machine Learning and Search Methods for S&P500 Stock Portfolio Forecasting and Optimization\n",
    "\n",
    " Project developed by: **Eduardo Passos** [202205630](https://sigarra.up.pt/fcup/pt/fest_geral.cursos_list?pv_num_unico=202205630), **Pedro Fernandes** [202208347](https://sigarra.up.pt/fcup/pt/fest_geral.cursos_list?pv_num_unico=202208347) and **Rafael Pacheco** [202206258](https://sigarra.up.pt/fcup/pt/fest_geral.cursos_list?pv_num_unico=202206258)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index {#index} #############################################\n",
    "1. [Project Introduction](#intro)\n",
    "2. [Introduction to Stock Concepts](#intro2)\n",
    "3. [Data Extraction and Collection](#data)\n",
    "\n",
    "    3.1 [Visualization and Formatting](#vis)\n",
    "4. [Exploratory Data Analysis](#eda)\n",
    "\n",
    "5. \n",
    "\n",
    "? [Conclusion](#conclusion)\n",
    "? [References](#ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Introduction and Motivation {#intro}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project encompasses the creation of a well suited investment strategy based on the S&P500 stock dataset.\n",
    "The highlights of the group's development process are detailed throughout this report. All of the specific files utilized during the project's development can be found inside the submitted folder.\n",
    "\n",
    "In order to predict stock behaviour, we employed:\n",
    "\n",
    " - `Deep Learning`: Long Short-Term Memory (LSTM)\n",
    " - ...\n",
    "\n",
    "To optimize portfolio selection, we implemented:\n",
    "\n",
    " - `Search Methods`: Monte Carlo Tree Search (MCTS)\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stock market is highly volatile and unpredictable, making stock price prediction nearly luck based.\n",
    "\n",
    "In order to create strategies that allow for investors to efficiently obtain risk-adjusted returns, we can use **S&P500 data** to get a better understanding of how the stock market may behave, based on previously collected data and statistics.\n",
    "\n",
    "It's important to mention that it doesn't always follow a guaranteed predictable, mathematical pattern. It is influenced by many real-world factors, independent to a company's growth and significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Stock Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the reader is unfamiliar with stocks and investing, we decided to briefly explain key concepts used throughout this report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The S&P500 is a stock market index that tracks the performance of 500 of the largest publicly traded companies in the United States. As per requested in the project statement, we used this dataset's information, from 2010 to 2023, in order to predict the stock behaviour of those companies during January 20204."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What are stocks, and why are they an investment?**\n",
    "Stocks (or shares) represent ownership in a company. Investors buy stocks to gain a portion of a company's profits, or to benefit from an increase in the stock's market value.\n",
    "\n",
    "#### **What are tickers?**\n",
    "A ticker is a unique symbol assigned to a company's stock, essentially an identifier for each company, in order to facilitate stock tracking:\n",
    "\n",
    " - `AAPL`: Apple Inc.\n",
    " - `GOOG`: Alphabet Inc. (or, simply put, Google)\n",
    " \n",
    "### **What are opening and closing prices?**\n",
    "The opening price is the price at which a stock begins trading, when the market opens for the day. \n",
    "The closing price is the actual last transaction price on that day, for that specific stock.\n",
    "\n",
    "We will be using daily windows in order to predict these prices.\n",
    "\n",
    "### **What are windows, and how are they helpful during prediction?**\n",
    "In time series analysis, a \"window\" refers to a segment of the data used for analysis or prediction. \n",
    "By using time series analysis, we aim to identify patterns, trends, and seasonal effects in the data.\n",
    "\n",
    "### **Most importantly, how can I gain or lose money by investing?**\n",
    "\n",
    "A positive return indicates profit, while a negative return signifies a loss. \n",
    "These are typically expressed as a percentage of the original investment. \n",
    "\n",
    "Imagine the investor purchases stock at 100:\n",
    "\n",
    " - Stock price increases from 100 to 110 -> the return is 10% -> <span style=\"color:green\">Profit!</span>\n",
    "\n",
    " - Stock price decreases from 100 to 90 -> the return is -10% -> <span style=\"color:red\">Loss!</span>\n",
    "\n",
    "Market fluctuations dictate stock prices, which in return represent profit or loss for investors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction and Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract the 2010-2023 section of the dataset, we used the `yfinance` module. \n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies) is also accessed to download a table containing a list of S&P500 tickers.\n",
    "\n",
    "The functions below document the extraction and collection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the list of S&P 500 companies\n",
    "def get_sp500_tickers():\n",
    "    # Download the table from Wikipedia\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    table = pd.read_html(url)[0]\n",
    "    tickers = table['Symbol'].tolist()\n",
    "    \n",
    "    # Remove any invalid ticker symbols if necessary\n",
    "    tickers = [ticker.replace('.', '-') for ticker in tickers]  # For Yahoo Finance compatibility\n",
    "    return tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Download data for each stock\n",
    "def download_sp500_data(tickers, start_date=\"2010-01-01\", end_date=\"2024-12-31\", group_by=\"ticker\"):\n",
    "    data = {}\n",
    "    for ticker in tickers:\n",
    "        print(f\"Downloading data for {ticker}...\")\n",
    "        try:\n",
    "            data[ticker] = yf.download(ticker, start=start_date, end=end_date)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {ticker}: {e}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Save or analyze the data\n",
    "\n",
    "raw_directory = \"raw_csvs\"  # Specify the directory where you want to save the files\n",
    "\n",
    "def save_data_to_csv(data):\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(raw_directory):\n",
    "        os.makedirs(raw_directory)\n",
    "    \n",
    "    for ticker, df in data.items():\n",
    "        if not df.empty:\n",
    "            # Save the file in the specified directory\n",
    "            file_path = os.path.join(raw_directory, f\"{ticker}.csv\")\n",
    "            df.to_csv(file_path)\n",
    "            print(f\"Data for {ticker} saved to {file_path}.\")\n",
    "        else:\n",
    "            print(f\"No data for {ticker}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to run. (dps muda se tava farto de tirar e meter acentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py \n",
    "\n",
    "tickers = get_sp500_tickers()\n",
    "sp500_data = download_sp500_data(tickers)\n",
    "save_data_to_csv(sp500_data)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Formatting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data we extracted. We'll use the Tesla stocks, `TSLA`, for showcasing formats and changes, since each csv was extracted equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-06-29 00:00:00+00:00</td>\n",
       "      <td>1.5926669836044312</td>\n",
       "      <td>1.5926669836044312</td>\n",
       "      <td>1.6666669845581055</td>\n",
       "      <td>1.1693329811096191</td>\n",
       "      <td>1.2666670083999634</td>\n",
       "      <td>281494500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-06-30 00:00:00+00:00</td>\n",
       "      <td>1.5886670351028442</td>\n",
       "      <td>1.5886670351028442</td>\n",
       "      <td>2.0280001163482666</td>\n",
       "      <td>1.553333044052124</td>\n",
       "      <td>1.7193330526351929</td>\n",
       "      <td>257806500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-07-01 00:00:00+00:00</td>\n",
       "      <td>1.4639999866485596</td>\n",
       "      <td>1.4639999866485596</td>\n",
       "      <td>1.7280000448226929</td>\n",
       "      <td>1.3513330221176147</td>\n",
       "      <td>1.6666669845581055</td>\n",
       "      <td>123282000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Price           Adj Close               Close  \\\n",
       "0                     Ticker                TSLA                TSLA   \n",
       "1                       Date                 NaN                 NaN   \n",
       "2  2010-06-29 00:00:00+00:00  1.5926669836044312  1.5926669836044312   \n",
       "3  2010-06-30 00:00:00+00:00  1.5886670351028442  1.5886670351028442   \n",
       "4  2010-07-01 00:00:00+00:00  1.4639999866485596  1.4639999866485596   \n",
       "\n",
       "                 High                 Low                Open     Volume  \n",
       "0                TSLA                TSLA                TSLA       TSLA  \n",
       "1                 NaN                 NaN                 NaN        NaN  \n",
       "2  1.6666669845581055  1.1693329811096191  1.2666670083999634  281494500  \n",
       "3  2.0280001163482666   1.553333044052124  1.7193330526351929  257806500  \n",
       "4  1.7280000448226929  1.3513330221176147  1.6666669845581055  123282000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./raw_csvs/TSLA.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Price', 'Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is presented in an unusual format. In order to facilitate analysis and training, we will be converting it into something more familiar, taking into consideration that:\n",
    " - Ticker is consistent for each csv;\n",
    " - `Price` can be overwritten as `Date`;\n",
    " - Removing NaN from column 1 will finish the process;\n",
    "\n",
    "The function below applies this process to any given dataset from the `raw_csvs` folder, transforming it into a clean, working dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done by opening each `csv` file and converting it to a dataframe using `pandas`. Then, the irrelevant rows (0 contains tickers, and 1 contains NaN) are dropped. The `Price` column is in place of `Date`, therefore, `Price` is changed to `Date`. Finally, the date + time format is modified to `YYYY-MM-DD` format, making sure we only keep relevant information. We also transform each string column into `float64`, except for `Volume`, which is always an Integer.\n",
    "\n",
    "\n",
    "Optionally, we can keep only the `Adj Close` column for each stock, which is the adjusted closing price, and ultimatelly our target for prediction. If the argument **onlyAdj** is kept as `False`, then all of the clean columns are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(file, onlyAdj=False):\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Drop rows with index 0 and 1, weird download format\n",
    "    df = df.drop([0, 1])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df.rename(columns={'Price': 'Date', 'Adj Close': 'AdjClose'}, inplace=True) \n",
    "    df.rename(columns={'Adj Close': 'AdjClose'}, inplace=True) \n",
    "    \n",
    "    # Get only date\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.date \n",
    "    \n",
    "    if onlyAdj:\n",
    "        # Keeps only the 'Date' and Adjusted Close ('Adj Close') columns\n",
    "        df = df[['Date', 'Adj Close\t']] \n",
    " \n",
    "    # Set 'Date' as the index \n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Reset the index so that 'Date' is no longer the index\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Finally, convert strings to float\n",
    "    for col in [\"AdjClose\", \"Close\", \"Open\", \"High\", \"Low\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Check the current columns to see what needs to be dropped (debug)\n",
    "    #print(df.columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-06-29</td>\n",
       "      <td>1.5926669836044312</td>\n",
       "      <td>1.5926669836044312</td>\n",
       "      <td>1.6666669845581055</td>\n",
       "      <td>1.1693329811096191</td>\n",
       "      <td>1.2666670083999634</td>\n",
       "      <td>281494500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>1.5886670351028442</td>\n",
       "      <td>1.5886670351028442</td>\n",
       "      <td>2.0280001163482666</td>\n",
       "      <td>1.553333044052124</td>\n",
       "      <td>1.7193330526351929</td>\n",
       "      <td>257806500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-07-01</td>\n",
       "      <td>1.4639999866485596</td>\n",
       "      <td>1.4639999866485596</td>\n",
       "      <td>1.7280000448226929</td>\n",
       "      <td>1.3513330221176147</td>\n",
       "      <td>1.6666669845581055</td>\n",
       "      <td>123282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-07-02</td>\n",
       "      <td>1.2799999713897705</td>\n",
       "      <td>1.2799999713897705</td>\n",
       "      <td>1.5399999618530273</td>\n",
       "      <td>1.24733304977417</td>\n",
       "      <td>1.5333329439163208</td>\n",
       "      <td>77097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-07-06</td>\n",
       "      <td>1.0740000009536743</td>\n",
       "      <td>1.0740000009536743</td>\n",
       "      <td>1.3333330154418945</td>\n",
       "      <td>1.0553330183029175</td>\n",
       "      <td>1.3333330154418945</td>\n",
       "      <td>103003500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date           Adj Close               Close                High  \\\n",
       "0  2010-06-29  1.5926669836044312  1.5926669836044312  1.6666669845581055   \n",
       "1  2010-06-30  1.5886670351028442  1.5886670351028442  2.0280001163482666   \n",
       "2  2010-07-01  1.4639999866485596  1.4639999866485596  1.7280000448226929   \n",
       "3  2010-07-02  1.2799999713897705  1.2799999713897705  1.5399999618530273   \n",
       "4  2010-07-06  1.0740000009536743  1.0740000009536743  1.3333330154418945   \n",
       "\n",
       "                  Low                Open     Volume  \n",
       "0  1.1693329811096191  1.2666670083999634  281494500  \n",
       "1   1.553333044052124  1.7193330526351929  257806500  \n",
       "2  1.3513330221176147  1.6666669845581055  123282000  \n",
       "3    1.24733304977417  1.5333329439163208   77097000  \n",
       "4  1.0553330183029175  1.3333330154418945  103003500  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = process_csv('./raw_csvs/TSLA.csv')\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to **automate this process**, we can get a list of tickers and save clear data to the directory below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_sp500_tickers()\n",
    "tickers.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_directory = 'clean_csvs'\n",
    "\n",
    "def save_cleaned_data(tickers, raw_dir, clean_dir):\n",
    "    if not os.path.exists(clean_dir):\n",
    "        os.makedirs(clean_dir)\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        raw_file_path = os.path.join(raw_dir, f\"{ticker}.csv\")\n",
    "        #print(raw_file_path)\n",
    "        if os.path.exists(raw_file_path):\n",
    "            # Process the CSV file\n",
    "            #print(f\"Processing {ticker}...\")\n",
    "            df = process_csv(raw_file_path)\n",
    "            \n",
    "            if not df.empty:\n",
    "                # Save the processed DataFrame to the clean directory\n",
    "                clean_file_path = os.path.join(clean_dir, f\"{ticker}.csv\")\n",
    "                df.to_csv(clean_file_path, index=False)\n",
    "                #print(f\"Data for {ticker} saved to {clean_file_path}.\")\n",
    "            else:\n",
    "                #print(f\"No valid data for {ticker}. Skipping...\")\n",
    "                pass\n",
    "        else:\n",
    "            #print(f\"File for {ticker} not found in {raw_dir}. Skipping...\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running this line of code, every `csv` file in the `raw_csv` directory will be read and its new, formatted contents will be written to a new `csv` file in the `clean_csv` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "save_cleaned_data(tickers, raw_directory, clean_directory)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is presented in a familiar format, we can begin to analyze the `csv` contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to check if all files' data types are consistent with one anoteher, and verify if any of them possess any NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<string>:4\u001b[1;36m\u001b[0m\n\u001b[1;33m    for ticker in tickers:\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "has_nan = [] \n",
    "\n",
    "# Iterate over each file \n",
    "for ticker in tickers:\n",
    "    file_path = os.path.join(clean_directory, f\"{ticker}.csv\")\n",
    "\n",
    "    # Temporarily transform into dataframe\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp500",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
